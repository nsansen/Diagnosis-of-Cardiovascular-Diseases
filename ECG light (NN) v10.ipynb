{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, label_ranking_average_precision_score, label_ranking_loss, coverage_error \n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from scipy.signal import resample\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "import pickle\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import itertools\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Conv1D, MaxPooling1D, Softmax, Add, Flatten, Activation# , Dropout\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  # DATA ACQUISITION *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"C:/Users/nisch/Desktop/ECG Categorization\\DATASET\\mitbih_test.csv\", header=None);\n",
    "train = pd.read_csv(\"C:/Users/nisch/Desktop/ECG Categorization\\DATASET\\mitbih_train.csv\", header=None);\n",
    "mit_train_data = train\n",
    "mit_test_data = test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRODUCE BALANCED DATASET train_df , test_df *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nisch\\anaconda3\\envs\\PythonGPU\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "C:\\Users\\nisch\\anaconda3\\envs\\PythonGPU\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "C:\\Users\\nisch\\anaconda3\\envs\\PythonGPU\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "C:\\Users\\nisch\\anaconda3\\envs\\PythonGPU\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "C:\\Users\\nisch\\anaconda3\\envs\\PythonGPU\\lib\\site-packages\\ipykernel_launcher.py:23: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0    20000\n",
      "3.0    20000\n",
      "4.0    20000\n",
      "2.0    20000\n",
      "0.0    20000\n",
      "Name: 187, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# There is a huge difference in the balanced of the classes.\n",
    "# Better choose the resample technique more than the class weights for the algorithms.\n",
    "from sklearn.utils import resample\n",
    "\n",
    "df_1=mit_train_data[mit_train_data[187]==1]\n",
    "df_2=mit_train_data[mit_train_data[187]==2]\n",
    "df_3=mit_train_data[mit_train_data[187]==3]\n",
    "df_4=mit_train_data[mit_train_data[187]==4]\n",
    "df_0=(mit_train_data[mit_train_data[187]==0]).sample(n=20000,random_state=42)\n",
    "\n",
    "df_1_upsample=resample(df_1,replace=True,n_samples=20000,random_state=123)\n",
    "df_2_upsample=resample(df_2,replace=True,n_samples=20000,random_state=124)\n",
    "df_3_upsample=resample(df_3,replace=True,n_samples=20000,random_state=125)\n",
    "df_4_upsample=resample(df_4,replace=True,n_samples=20000,random_state=126)\n",
    "\n",
    "train_df=pd.concat([df_0,df_1_upsample,df_2_upsample,df_3_upsample,df_4_upsample])\n",
    "\n",
    "\n",
    "df_11=mit_test_data[mit_train_data[187]==1]\n",
    "df_22=mit_test_data[mit_train_data[187]==2]\n",
    "df_33=mit_test_data[mit_train_data[187]==3]\n",
    "df_44=mit_test_data[mit_train_data[187]==4]\n",
    "df_00=(mit_test_data[mit_train_data[187]==0]).sample(n=20000,random_state=42)\n",
    "\n",
    "df_11_upsample=resample(df_1,replace=True,n_samples=20000,random_state=123)\n",
    "df_22_upsample=resample(df_2,replace=True,n_samples=20000,random_state=124)\n",
    "df_33_upsample=resample(df_3,replace=True,n_samples=20000,random_state=125)\n",
    "df_44_upsample=resample(df_4,replace=True,n_samples=20000,random_state=126)\n",
    "\n",
    "test_df=pd.concat([df_00,df_11_upsample,df_22_upsample,df_33_upsample,df_44_upsample])\n",
    "\n",
    "\n",
    "equilibre=train_df[187].value_counts()\n",
    "print(equilibre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL Train data\n",
      "Type\tCount\n",
      "0.0    72471\n",
      "4.0     6431\n",
      "2.0     5788\n",
      "1.0     2223\n",
      "3.0      641\n",
      "Name: 187, dtype: int64\n",
      "-------------------------\n",
      "ALL Test data\n",
      "Type\tCount\n",
      "0.0    18118\n",
      "4.0     1608\n",
      "2.0     1448\n",
      "1.0      556\n",
      "3.0      162\n",
      "Name: 187, dtype: int64\n",
      "ALL Balanced Train data\n",
      "Type\tCount\n",
      "1.0    20000\n",
      "3.0    20000\n",
      "4.0    20000\n",
      "2.0    20000\n",
      "0.0    20000\n",
      "Name: 187, dtype: int64\n",
      "-------------------------\n",
      "ALL Balanced Test data\n",
      "Type\tCount\n",
      "1.0    20000\n",
      "3.0    20000\n",
      "4.0    20000\n",
      "2.0    20000\n",
      "0.0    20000\n",
      "Name: 187, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"ALL Train data\")\n",
    "print(\"Type\\tCount\")\n",
    "print((mit_train_data[187]).value_counts())\n",
    "print(\"-------------------------\")\n",
    "print(\"ALL Test data\")\n",
    "print(\"Type\\tCount\")\n",
    "print((mit_test_data[187]).value_counts())\n",
    "\n",
    "print(\"ALL Balanced Train data\")\n",
    "print(\"Type\\tCount\")\n",
    "print((train_df[187]).value_counts())\n",
    "print(\"-------------------------\")\n",
    "print(\"ALL Balanced Test data\")\n",
    "print(\"Type\\tCount\")\n",
    "print((train_df[187]).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONE HOT Encoding *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One hot encoding for categorical target\n",
    "#Since we will be using neural networks for our classification model, \n",
    "#our output classes need to be turned into a numerical representation. We use one hot encoding (from sklearn package) to do this.\n",
    "\n",
    "\n",
    "\n",
    "#train_target = mit_train_data[187]\n",
    "#train_target = train_target.values.reshape(87554,1)\n",
    "train_target = train_df[187]\n",
    "train_target = train_target.values.reshape(100000,1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#one hot encode train_target\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import preprocessing\n",
    "# TODO: create a OneHotEncoder object, and fit it to all of X\n",
    "\n",
    "# 1. INSTANTIATE\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "\n",
    "# 2. FIT\n",
    "enc.fit(train_target)\n",
    "\n",
    "# 3. Transform\n",
    "onehotlabels = enc.transform(train_target).toarray()\n",
    "onehotlabels.shape\n",
    "\n",
    "target = onehotlabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75000, 187, 1)\n",
      "(75000, 5)\n"
     ]
    }
   ],
   "source": [
    "#remove ground truth labels from training df\n",
    "#train/test split\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#X = mit_train_data\n",
    "X = train_df\n",
    "X = X.drop(axis=1,columns=187)\n",
    "\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X,target, test_size = 0.25, random_state = 36)\n",
    "X_train = np.asarray(X_train)\n",
    "X_valid = np.asarray(X_valid)\n",
    "Y_train = np.asarray(Y_train)\n",
    "Y_valid = np.asarray(Y_valid)\n",
    "\n",
    "#X_train.reshape((1, 2403, 187))\n",
    "X_train = np.expand_dims(X_train, axis=2)\n",
    "X_valid = np.expand_dims(X_valid, axis=2)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "# 2,403 training heartbeats and 802 validation heartbeats \n",
    "# for a 75:25 train-test split. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 MODEL NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- X ---\n",
      "            0         1         2         3         4         5         6    \\\n",
      "10153  0.162791  0.540698  0.755814  0.186047  0.168605  0.546512  0.616279   \n",
      "33886  0.990066  0.938742  0.344371  0.034768  0.273179  0.331126  0.326159   \n",
      "32005  0.974239  0.932084  0.590164  0.131148  0.014052  0.168618  0.238876   \n",
      "56159  0.978495  0.723118  0.526882  0.298387  0.220430  0.158602  0.091398   \n",
      "61783  0.963351  0.709424  0.060209  0.013089  0.057592  0.041885  0.047120   \n",
      "\n",
      "            7         8         9    ...  177  178  179  180  181  182  183  \\\n",
      "10153  0.697674  0.651163  0.703488  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "33886  0.341060  0.347682  0.347682  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "32005  0.210773  0.196721  0.208431  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "56159  0.091398  0.080645  0.083333  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "61783  0.034031  0.039267  0.044503  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "\n",
      "       184  185  186  \n",
      "10153  0.0  0.0  0.0  \n",
      "33886  0.0  0.0  0.0  \n",
      "32005  0.0  0.0  0.0  \n",
      "56159  0.0  0.0  0.0  \n",
      "61783  0.0  0.0  0.0  \n",
      "\n",
      "[5 rows x 187 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 100000 entries, 10153 to 86696\n",
      "Columns: 187 entries, 0 to 186\n",
      "dtypes: float64(187)\n",
      "memory usage: 143.4 MB\n",
      "None\n",
      "--- Y ---\n",
      "--- testX ---\n",
      "            0         1         2         3         4         5         6    \\\n",
      "17155  0.966581  0.802057  0.077121  0.000000  0.161954  0.244216  0.254499   \n",
      "8157   1.000000  0.644444  0.031111  0.026667  0.151111  0.128889  0.080000   \n",
      "3635   1.000000  0.965318  0.734104  0.462428  0.271676  0.156069  0.150289   \n",
      "19125  1.000000  0.998792  0.960145  0.896135  0.898551  0.873188  0.766908   \n",
      "3002   0.009836  0.000000  0.163934  0.242623  0.327869  0.350820  0.331148   \n",
      "\n",
      "            7         8         9    ...  177  178  179  180  181  182  183  \\\n",
      "17155  0.262211  0.264782  0.272494  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "8157   0.080000  0.084444  0.080000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "3635   0.173410  0.173410  0.150289  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "19125  0.591787  0.327295  0.252415  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "3002   0.331148  0.354098  0.373771  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "\n",
      "       184  185  186  \n",
      "17155  0.0  0.0  0.0  \n",
      "8157   0.0  0.0  0.0  \n",
      "3635   0.0  0.0  0.0  \n",
      "19125  0.0  0.0  0.0  \n",
      "3002   0.0  0.0  0.0  \n",
      "\n",
      "[5 rows x 187 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 100000 entries, 17155 to 86696\n",
      "Columns: 187 entries, 0 to 186\n",
      "dtypes: float64(187)\n",
      "memory usage: 143.4 MB\n",
      "None\n",
      "--- testy ---\n"
     ]
    }
   ],
   "source": [
    "# MODEL 1 https://www.kaggle.com/freddycoder/heartbeat-categorization\n",
    "# Separate features and targets\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "print(\"--- X ---\")\n",
    "# X = mit_train_data.loc[:, mit_train_data.columns != 187]\n",
    "X = train_df.loc[:, mit_train_data.columns != 187]\n",
    "print(X.head())\n",
    "print(X.info())\n",
    "\n",
    "print(\"--- Y ---\")\n",
    "# y = mit_train_data.loc[:, mit_train_data.columns == 187]\n",
    "y = train_df.loc[:, mit_train_data.columns == 187]\n",
    "y = to_categorical(y)\n",
    "\n",
    "print(\"--- testX ---\")\n",
    "#testX = mit_test_data.loc[:, mit_test_data.columns != 187]\n",
    "testX = test_df.loc[:, mit_test_data.columns != 187]\n",
    "print(testX.head())\n",
    "print(testX.info())\n",
    "\n",
    "print(\"--- testy ---\")\n",
    "#testy = mit_test_data.loc[:, mit_test_data.columns == 187]\n",
    "testy = test_df.loc[:, mit_test_data.columns == 187]\n",
    "testy = to_categorical(testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " 90080/100000 [==========================>...] - ETA: 1s - loss: 0.4498 - accuracy: 0.8350"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-fb73c08fa62c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m               metrics=['accuracy'])\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m## SAVE MODEL ##\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonGPU\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonGPU\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonGPU\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3733\u001b[0m     return nest.pack_sequence_as(\n\u001b[0;32m   3734\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_outputs_structure\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3735\u001b[1;33m         \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3736\u001b[0m         expand_composites=True)\n\u001b[0;32m   3737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonGPU\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   3733\u001b[0m     return nest.pack_sequence_as(\n\u001b[0;32m   3734\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_outputs_structure\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3735\u001b[1;33m         \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3736\u001b[0m         expand_composites=True)\n\u001b[0;32m   3737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonGPU\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    906\u001b[0m     \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 908\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    909\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    910\u001b[0m       \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Keras model to make prediction\n",
    "\n",
    "#The number of epochs is a hyperparameter that defines the number times that the learning algorithm will work through the entire training dataset.\n",
    "#The batch size is a hyperparameter that defines the number of samples to work through before updating the internal model parameters.\n",
    "# softmax is used to categorize \n",
    "from keras.callbacks import History \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='accuracy', min_delta=0, patience=1, mode='max', baseline=None, restore_best_weights=False)\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(50, activation='relu', input_shape=(187,)))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='Adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X, y, epochs=20,verbose=1,callbacks=[early_stopping] )\n",
    "\n",
    "## SAVE MODEL ##\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"NN.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"NN.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "print(\"Evaluation: \")\n",
    "mse, acc = model.evaluate(testX, testy)\n",
    "print('mean_squared_error :', mse)\n",
    "print('accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "#plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "#plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARDUINO SAMPLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"C:/Users/nisch/Desktop/ECG Categorization/ARDUINO SAMPLES/test44.txt\", header=None)\n",
    "test = test.iloc[0,0:len(test.T)-1] # Remove last line cause it might be a Nan\n",
    "test = pd.DataFrame(test)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORMALIZING TEST DATA AMPLITUDE\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# load the dataset and print the first 5 rows\n",
    "# prepare data for normalization\n",
    "values = test.values\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler = scaler.fit(values)\n",
    "normalized = scaler.transform(values)\n",
    "normalized = pd.DataFrame(normalized.T) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TESTING FOR ONE X\n",
    "#x=0\n",
    "#normalized = pd.DataFrame(normalized.T) ## CAUTION!!! needs to run only once \n",
    "#normtest=normalized.iloc[0, 0+x:187+x] \n",
    "#normtest=pd.DataFrame(normtest)\n",
    "#category = model.predict_classes(normtest.T)\n",
    "#category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category= pd.DataFrame()\n",
    "category=category.dropna()\n",
    "lst_seq = np.arange(0,len(normalized.T)-190)\n",
    "for x in lst_seq:\n",
    "    normtest=normalized.iloc[0, 0+x:187+x] \n",
    "    normtest=pd.DataFrame(normtest)\n",
    "    category[x] = model.predict_classes(normtest.T)\n",
    "category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MEAN OF CATEGORIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(category.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLOT OF CATEGORIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(category.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display frequency of each predicted category as evaluated by model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = pd.DataFrame(category)\n",
    "temp1= category.iloc[0,:].value_counts()\n",
    "print(\"Categories vs Value Count\")\n",
    "print(temp1)\n",
    "print(\"Categories vs Frequency\")\n",
    "print(temp1/(len(category.T)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
